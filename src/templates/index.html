<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="{{ url_for('static', filename='css/tufte.css') }}">
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <title>MNIST Generator</title>
    <style>
        .loader {
            border: 5px solid #f3f3f3;
            border-radius: 50%;
            border-top: 5px solid blue;
            border-right: 5px solid green;
            border-bottom: 5px solid red;
            border-left: 5px solid pink;
            width: 16px;
            height: 16px;
            -webkit-animation: spin 2s linear infinite; /* Safari */
            animation: spin 2s linear infinite;
            display:inline;
            float:left;
        }

        /* Safari */
        @-webkit-keyframes spin {
            0% { -webkit-transform: rotate(0deg); }
            100% { -webkit-transform: rotate(360deg); }
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        .loading {
            display: inline;
            float: left;
            margin-top: 0;
            margin-right: 5px;
            margin-bottom: 95px;
        }

        #generate_button {
            font-family: et-book, Palatino, "Palatino Linotype", "Palatino LT STD", "Book Antiqua", Georgia, serif;
            background-color: #fffff8;
            color: #111;
            max-width: 1400px;
            counter-reset: sidenote-counter;
            font-size: 1.4rem;
            line-height: 2rem;
            padding: 5px 16px;
        }
</style>
</head>
<body>
    <h1>iGPT</h1>
    <p class="subtitle">David Earnest</p>

    <section>
        <p>This webpage hosts a simple autoregressive <a href="https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf">model</a> that I trained from scratch on the binary <a href="https://en.wikipedia.org/wiki/MNIST_database">MNIST</a> dataset.</p>
    </section>

    <figure>
        <label for="mnist_sample" class="margin-toggle">&#8853;</label><input type="checkbox" id="mnist_sample" class="margin-toggle"/><span class="marginnote">a digit generated from an iGPT model trained on binary MNIST.</span>
        <img src="{{ url_for('static', filename='images/' + image) }}" alt="a handwritten digit generated by iGPT" width="640" height="640">
    </figure>

    <form action="/" method="POST">
        <input type="submit" value="Generate" id="generate_button" onclick="displayLoading()" style="margin-bottom:50px">
    </form>
    <p class="loading" id="loading" style="display:none;">Generating...</p><div class="loader" id="loader" style="display:none;"></div>
    <br>
    <hr style="display:block; width:100%;">

    <section>
        <h2 style="display:block; width:100%;">Model Overview</h2>

        <p>
            iGPT is an autoregressive model similar to LLMs, except is it trained on images instead of text. In both cases we are given some dataset, \( \mathcal{D} = \{x_1, \ldots, x_n\} \), and our goal is to learn a model that can generate sample from the dataset generating distribution.
        </p>

        <p>
            If we formalized this as maximum likelihood estimation (MLE), then we want to find the underlying model parameters that maximize the log-likelihood of our data, i.e.,
            $$
            \begin{equation}
                \theta^* := \text{argmax}_\theta \sum_{i=1}^n \log p_\theta(x_i)
            \end{equation}
            $$
        </p>

        <p>
            The above formalization makes it difficult to sample from our learned model, but if we break our data points apart into tokens we can rewrite our objective as
            $$
            \begin{equation}
            \begin{aligned}
                \theta^* :&= \text{argmax}_\theta \sum_{i=1}^n \log p_\theta(x_i)\\
                &= \text{argmax}_\theta \sum_{i=1}^n \log p_\theta(x_{i1} \cap \ldots \cap x_{im})\\
                &= \text{argmax}_\theta \sum_{i=1}^n \sum_{j=1}^m \log p_\theta(x_{ij} | x_{i1}, \ldots x_{ij-1})
            \end{aligned}
            \end{equation}
            $$
        </p>

        <p>
            The above reformulation learns the parameters for the PDF of the next token given all the previous tokens. In the case of iGPT, the tokens correspond to pixels, so we learn the distribution over the next pixel given the previous pixels in the image (where pixels are ordered from left to right, top to bottom).
        </p>

        <p>
            Once we learn the parameters we can sample a new image. We start with a sequence begining with the special begining of sequence (&ltbos&gt) token<label for="sn-extensive-use-of-sidenotes" class="margin-toggle sidenote-number"></label><input type="checkbox" id="sn-extensive-use-of-sidenotes" class="margin-toggle"/><span class="sidenote">The &ltbos&gt token needs to be appended to all the data before training.</span> and by passing this sequence through the trained model, we can sample the first token/pixel, then the next, one at a time until a new image is generated.
        </p>

    </section>

    <section>
        <h2>Model Architecture</h2>

        <p>
            We use a model with two transformer decoder blocks, four attention heads, GeLU nonlinearities, and learned positional encodings. See <a href="https://arxiv.org/pdf/1706.03762">Attention Is All You Need</a> for more details.
        </p>
    </section>


<script>
    function displayLoading() {
        document.getElementById("loader").style.display = "block";
        document.getElementById("loading").style.display = "block";
        document.getElementById("generate_button").style.display = "none";
}
</script>
</body>
</html>